{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes DRS batch file (.csv) and returns absolute filepaths (.txt) for Handprint + OCR lookup table (.csv)\n",
    "##Matt Cook - 2021\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#inputs\n",
    "tableIn = pd.read_csv((\".../Nextcloud/Houghton/SAEF/2_ready/afam_fy21_batch0008/HOU-afam_fy21_batch0008_sorted.csv\"), usecols=['FILE-OSN']) ##batch info csv\n",
    "target =\".../Nextcloud/Houghton/SAEF/2_ready/afam_fy21_batch0008\" #target image directory\n",
    "\n",
    "#outputs\n",
    "pathsOut = open(\".../Nextcloud/Houghton/SAEF/3_inprogress/OCR_Paths.txt\", \"w\") # for Handprint\n",
    "tableOut = \".../Nextcloud/Houghton/SAEF/3_inprogress/OCR_LookupTable.csv\"# transcription batch csv\n",
    "\n",
    "#dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#cross-check that DRS FILE-OSN values exist in target directory and add matches to pathsOut + dataframe\n",
    "for path in sorted(Path(target).rglob('*.jpg')):\n",
    "    if path.stem in tableIn.values: #check if image exists in batch info\n",
    "        absolute = (str(path.parent) + \"/\" + path.name) #absolute path for images\n",
    "        pathsOut.write(str(absolute)) #write paths to pathsOut\n",
    "        pathsOut.write(\"\\n\")\n",
    "        df = df.append({'FILE-OSN':path.stem,'IMG-PATH':absolute}, ignore_index=True) #append data frame  \n",
    "        print(\"image \" + path.stem + \" located \" + \"at \" + absolute) #console out\n",
    "\n",
    "#create new lookupt table from dataframe\n",
    "with open(tableOut, mode = 'w') as f:\n",
    "    df.to_csv(f,index=False) #append tableOut with FILE-OSN and IMG-PATH values\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"lookup table created for collection\")\n",
    "pathsOut.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Handprint using paths from local file\n",
    "##Mike Hucka designed and implemented Handprint beginning in mid-2018.\n",
    "##installation instructions at https://github.com/caltechlibrary/handprint\n",
    "\n",
    "##generate Microsoft results\n",
    "!handprint --service microsoft -@\".../Nextcloud/Houghton/SAEF/2_ready/OCR/OCR_log_MSFT.txt\" --from-file \".../Nextcloud/Houghton/SAEF/2_ready/OCR/OCR_Paths.txt\" --no-grid --extended --output \".../Nextcloud/Houghton/SAEF/2_ready/OCR/MSFT\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes preliminary lookup table (CSV) and outputs updated table with locations for Handprint outputs, by file type\n",
    "##Matt Cook - 2021\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#Declarations\n",
    "table = \".../Nextcloud/Houghton/SAEF/2_ready/OCR/OCR_LookupTable.csv\" ##Lookup table\n",
    "hpOut =\".../Nextcloud/Houghton/SAEF/2_ready/OCR/MSFT\"##HandPrint Outputs\n",
    "types = ['MSFT-IMG','MSFT-JSON','MSFT-TXT']##additional lookup table columns\n",
    "suffX = ['.png','.json','.txt']#OCR file types\n",
    "num = 0 ##counter\n",
    "df = [1, 2, 3] #empty dataframes\n",
    "\n",
    "#match DRS \"FILE-OSN\" to OCR outputs and append new dataframes \n",
    "while num < len(df):  \n",
    "    df[num] = pd.DataFrame()\n",
    "    for path in sorted(Path(hpOut).rglob('*' + str(suffX[num]))): #Loop through file types\n",
    "        #define location and match to DRS's \"FILE-OSN\" column value\n",
    "        absolute = (str(path.parent) + \"/\" + path.name)\n",
    "        match = path.stem.split('.')\n",
    "        fileCore = pd.read_csv(table, usecols=['FILE-OSN'])\n",
    "        #append dataframe with matching values\n",
    "        for value in fileCore.values:\n",
    "            if value == match[0]:\n",
    "                df[num] = df[num].append({types[num]:absolute}, ignore_index=True)\n",
    "                print(\"\\n\")\n",
    "                print(\"dataframe \" + str(num + 1) + \" appended with \" + suffX[num] + \" located at \" + absolute)\n",
    "    num = num + 1   \n",
    "    \n",
    "#concatenate newly appended data frames with existing lookup table\n",
    "conC = pd.concat(df, axis=1)\n",
    "print(\"\\n\")\n",
    "print(\"new dataframes concatenated\")\n",
    "out = pd.concat([pd.read_csv(table),conC], axis=1)\n",
    "\n",
    "#update lookup table CSV\n",
    "with open(table, mode = 'w') as f:\n",
    "    out.to_csv(f,index=False, header=f.tell()==0) #append tableOut with FILE-OSN and IMG-PATH values\n",
    "    print(\"\\n\")\n",
    "    print(\"lookup table updated with HandPrint output locations for \" + str(types) + \"-type transcriptions\")\n",
    "    print(\"\\n\")\n",
    "    print(\"have a nice day\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Generates Bag-of-Words txt output from HandPrint transcriptions. \n",
    "###MNC - 5/21\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "#input/output\n",
    "paths = '.../Nextcloud/Houghton/SAEF/2_ready/OCR/OCR_Paths.txt'#plain text list of urls or filenames (pre-existing)\n",
    "target = '.../Nextcloud/Houghton/SAEF/2_ready/OCR/MSFT' #HP outputs (pre-existing)\n",
    "textOut = '.../Nextcloud/Houghton/SAEF/2_ready/OCR/MSFT/bagOfWords_MSFT.txt'#Bag-of-Words output\n",
    "\n",
    "#declarations\n",
    "BoW = open(textOut, \"a\")\n",
    "\n",
    "#append bag-of-words with headers and transcriptions\n",
    "for path in sorted(Path(target).rglob('*.txt')):\n",
    "    header = path.stem.split('.')\n",
    "    BoW.write(\"\\n\")\n",
    "    BoW.write(header[0]) #write headers location to bag-of-words\n",
    "    BoW.write(\"\\n\")\n",
    "    print((header[0]) + \" added to bag-of-words\")\n",
    "    absolute = (str(path.parent) + \"/\" + path.name)\n",
    "    contents = open(absolute, \"r\") \n",
    "    BoW.write(str(contents.read())) #write transcriptions to bag-of-words\n",
    "    print((\"Transcription for \" + header[0]) + \" added to bag-of-words\")\n",
    "    BoW.write(\"\\n\")\n",
    "BoW.close()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"have a nice day\")\n",
    "\n",
    "##add fuzzy search from https://github.com/marijnkoolen/fuzzy-search\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Fuzzy Search Bag-of-Words\n",
    "###https://github.com/marijnkoolen/fuzzy-search\n",
    "##Matt Cook - 2021\n",
    "\n",
    "from fuzzy_search.fuzzy_phrase_searcher import FuzzyPhraseSearcher\n",
    "from fuzzy_search.fuzzy_phrase_model import PhraseModel\n",
    "import json\n",
    "import re\n",
    "\n",
    "#declarations\n",
    "text1 = \"'.../Nextcloud/Houghton/SAEF/2_ready/OCR/MSFT/bagOfWords_MSFT.txt\"\n",
    "variants = []\n",
    "inputString = input(\"Fuzzy search document for keyword: \")\n",
    "terms = []\n",
    "terms.append(str(inputString))\n",
    "counter = 0\n",
    "\n",
    "#threshold configuration\n",
    "config = {\n",
    "    # these thresholds work when there are few OCR errors\n",
    "    'char_match_threshold': 0.5,\n",
    "    'ngram_threshold': 0.5,\n",
    "    'skipgram_threshold': 0.3,\n",
    "    'levenshtein_threshold': 0.5,\n",
    "    'include_variants': False, # for phrases that have variant phrasings\n",
    "    'filter_distractors': False, # avoid matching with similar but different phrases\n",
    "    \"ignorecase\": False, # Is upper/lowercase a meaningful signal?\n",
    "    \"use_word_boundaries\": False,# should matches follow word boundaries?\n",
    "    \"max_length_variance\": 3, # matching string can be lower/shorter than prhase\n",
    "}\n",
    "\n",
    "# initialize a new searcher instance with the config\n",
    "fuzzy_searcher = FuzzyPhraseSearcher(config)\n",
    "phrase_model = PhraseModel(phrases=terms)\n",
    "fuzzy_searcher.index_phrase_model(phrase_model)\n",
    "\n",
    "#identify matches in the text\n",
    "BoW = open(text1, \"r\")\n",
    "for match in fuzzy_searcher.find_matches(BoW.read()):\n",
    "    variant = match.json()\n",
    "    variant = variant['string']\n",
    "    variants.append(variant)\n",
    "print(\"Variants detected in bag-of-words include: \")\n",
    "print(variants)\n",
    "BoW.close()\n",
    "\n",
    "print(\"have a nice day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Named entity recognition and visualization from bag-of-transcriptions\n",
    "##Modified from \"named-entity-recognition\" repo by Mary Chester-Kadwell (https://github.com/mchesterkadwell/named-entity-recognition/blob/main/LICENSE)\n",
    "#Entity types: https://github.com/mchesterkadwell/named-entity-recognition/blob/main/2-named-entity-recognition-of-henslow-data.ipynb\n",
    "##Matt Cook - November 2021\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "#declarations\n",
    "nlp = en_core_web_sm.load()\n",
    "text_file = Path('data', '.../Nextcloud/Houghton/SAEF/2_ready/OCR/bagOfWords_MSFT.txt')\n",
    "entOut = open(\".../Nextcloud/Houghton/SAEF/2_ready/OCR/entities_PERSON.txt\", \"w\")\n",
    "    \n",
    "###named entity recognition\n",
    "with open(text_file, encoding=\"utf-8\") as file:\n",
    "    iliad = file.read()\n",
    "document = nlp(iliad)\n",
    "document.text\n",
    "entities = []\n",
    "for entity in document.ents:\n",
    "    if entity.label_ == \"PERSON\": \n",
    "        entities.append(entity.text)\n",
    "        print(entity.text)\n",
    "entOut.write(str(entities))\n",
    "displacy.render(document, style=\"ent\")\n",
    "        \n",
    "#print high-frenquency entities\n",
    "word_freq = Counter(entity)\n",
    "common_words = word_freq.most_common(25)\n",
    "print(common_words)\n",
    "\n",
    "#Display the plot in the notebook with interactive controls and save plot to disk\n",
    "%matplotlib notebook\n",
    "words = [word for word,_ in common_words]\n",
    "freqs = [count for _,count in common_words]\n",
    "plt.title(\"Named Entities\")\n",
    "plt.xlabel(\"Entity type\")\n",
    "plt.ylabel(\"# of appearances\")\n",
    "plt.xticks(range(len(words)), [str(s) for s in words], rotation=90)\n",
    "plt.grid(b=True, which='major', color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.gcf().subplots_adjust(bottom=0.35)\n",
    "plt.plot(freqs)\n",
    "plt.show()\n",
    "plt.savefig('.../Nextcloud/Houghton/SAEF/2_ready/OCR/personsGraph.png', bbox_inches=\"tight\")\n",
    "\n",
    "#close files\n",
    "file.close()\n",
    "entOut.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
